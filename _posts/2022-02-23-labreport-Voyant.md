---
layout: post
title: Lab 04 - Exploratory Data & Voyant
subtitle: Possbilities and Limits of Quantitative Text Data
gh-repo: daattali/beautiful-jekyll
gh-badge: [star, fork, follow]
tags: [lab report]
---

## Analyzing Tools in Voyant

1. Now that no words are being eliminated from the word cloud, I see mostly grammatical words. This is not useful interpreting the greater meaning of the collections of texts. I want to say that in most cases where the aim of analysis is in regard to themes, patterns of signification, and relationship between subjects, stop words are not very useful. However, if analysis instead becomes more linguistic in the sense of understanding, how, functionally and behaviorally, language is being used (“how often do users of X site buffer with “like,” “um,” “but,” etc), it seems stop words would provide important value. Further, one might enable stop word measures in their own writing or the writing of colleagues or students, for reflective and editing purposes (“hey, we use this connecting word too often”). 
2. Relative frequency tells us how much each word appears in repetition per X words (here one million). Per the example in class, it also tells us the proportion of percentage of the total number of words one single word occupies. This measure appears to have the power to better represent the space each word occupies in relation to others. In this way, it might also seem more predictive; if we know that a certain word appears at 26% in one paper on a niche topic, we might guess that it will make an appearance to some extent in another paper on the same topic, especially the more discipline-specific the word of interest. 
3. 
* Upon calculating TF-IDF scores (labeled “significance”) and isolating “MCAS,” we discover in class that the clustered frequency of MCAS comes from  is a grievance document.   
* It does not have any interesting connotative power because it is simply the name of the college that each person being named in the document attends, attended, or is employed within. I believe Kate said that this value might be useful for highlighting anomalies and I certainly agree. It seems that it might also be useful for process-of-elimination. In this case, because the TD-IDF score allowed us to understand that MCAS is not, actually, that significant, we can discard it from our larger study. 
	 
4. While the terms that are represented by this comparative value to appear more frequently in the science corpus seem to adhere to maybe more to research frameworks, those that occur more frequently in the humanities corpus seem more general. So, where the science corpus sees more occurrence of “science,” “new,” “study,” “research,” “human,” and “change,” the humanities corpus is more populated with words like “students,” “said,” studies,” “college,” “education,” and “history.” This information is actually quite suggestive. It seems that the binary under which the organizes of the corpora were operating saw a sort meta-writing as humanities oriented (writing about events on campuses and the politics of academia) and research-oriented writing as more scientific.
5.  I browsed Voyant’s [Tool's Guide](https://voyant-tools.org/docs/#!/guide/tools) and was intrigued by the TextualArc feature, which is defined by Voyant as “a visualization of the terms in a document that includes a weighted centroid of terms and an arc that follows the terms in document order.” To activate this tool, I hovered above the top-center panel, went down to “Visualization Tools,” then select “TextualArc.” I then exported this particular visualization to get a better view in a new tab. This tool effectively represents the corpora as a network and represents words as actors that have varying and winding relations. It showcases at the forefront the most powerful actors (the most frequently occurring words) and how they relate to and engender the use of other, minor words/actors across documents. (That said, we might take multiple viewpoints of who is the actual “actor” here; maybe it is the author, maybe it is the interface. For textual analysis of this sort, though, I’d like to say that actors = words and corpora = larger network.) In getting to this framework I am loosely borrowing from the work of Bruno Latour, Stan Ruecker, and Sydney J. Shep, featured in *A New Companion to Digital Humanities* (which I made my dad dig out of a giant stack of old text books and send to me... thanks dad). 

	In using TextualArc, I was maybe hoping for some sort of grand or causal narrative of the relationship between common words and less repeated words. I found instead that the provocation of minor words was always situational: there was no "humanities" is likely to trigger "X". That said, I did realize that in most cases, when a path of relation (chain of links) ultimately made it to a periphery word, it darted quickly back to the center, rather than engendering the use of other, less commonly used words. This, I suppose, is how language works? But it was fun to isolate at least one discernible and commonly repeated movement. 

## Reflections/Connections

6. Playing with different tools in Voyant and understanding their benefits and drawbacks definitely brought to mind D’Ignazio and Klein’s “The Numbers Don’t Speak for Themselves.” Before approaching this lab, When D’Iganzio and Klein talked about the frequent inability of decontextualized numbers (data) to make reliable claim, I agreed along the lines of social research, of course, but was not necessarily thinking in terms of textual data. The way in which social (psychological, sociological, anthropological) data is presented frames the way in which social structures and networks are either brought to light or ignored. But, all the same, framing text data in terms of questions _*how much?* _or *how many* can evade questions of  *why?*. 

	Reflecting on my own missteps in text data analysis: when I used Voyant in the past, I usually used it in the most simplistic way possible. That is, I often started by asking what terms appeared most frequently and then assuming those words indexed larger themes. Or, I used it specifically to search the frequency of niche words in less familiar texts to see if said texts might be useful to particular analyses. Before I had much familiarity with Voyant, I used a platform that was called *Textalyser* (I cannot find the version I used to use online anymore! I think it was bought out or reappropriated by some other organization), and relied on its proffered, decontextualized values for lexical scores and semantic value (connotative weight) to conduct comparative analyses. 

	D’Iganzio and Klein, in their various examples, urge not that using numerical data in the humanities is bad, *but* that numerical data is almost invaluable without investigation of context. What I learned from their suggestions in conjunction with exploring the affordances of Voyant in more depth, is that using quantitative data should not be thought of as short cut (and I think this was alluded to in class) *even* in the case of something that seems like it lower stakes than sociological data. 
